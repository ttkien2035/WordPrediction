{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TestModel.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO2Klj0/tU+MvP9noxz+03i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vomXevcjrVKw","executionInfo":{"status":"ok","timestamp":1617989594709,"user_tz":-420,"elapsed":25424,"user":{"displayName":"Tran Trung Kien","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBVSZw8E8uaIf28-VmePyDmzh9gvFKysWrDrAe=s64","userId":"13050720104544302355"}},"outputId":"4c56f276-0a65-4b19-f0fa-688f93f9e38f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6Hh-qlxzroug","executionInfo":{"status":"ok","timestamp":1617989836187,"user_tz":-420,"elapsed":760,"user":{"displayName":"Tran Trung Kien","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBVSZw8E8uaIf28-VmePyDmzh9gvFKysWrDrAe=s64","userId":"13050720104544302355"}}},"source":["import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kc7nm55brtth","executionInfo":{"status":"ok","timestamp":1617989725784,"user_tz":-420,"elapsed":1676,"user":{"displayName":"Tran Trung Kien","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBVSZw8E8uaIf28-VmePyDmzh9gvFKysWrDrAe=s64","userId":"13050720104544302355"}},"outputId":"a8f95a49-78c3-4054-ea80-3a08c2ebe47e"},"source":["# load ascii text and covert to lowercase\n","filename=open('/content/drive/MyDrive/IT_documents_University/Subjects/F3_semester_ThirdYear/DeepLearning/WordPrediction/data_vtc_giao_duc.txt',encoding=\"utf8\")\n","\n"," \n","\n","raw_text = filename.read()\n","raw_text = raw_text.lower()\n","# create mapping of unique chars to integers\n","chars = sorted(list(set(raw_text)))\n","char_to_int = dict((c, i) for i, c in enumerate(chars))\n","int_to_char = dict((i, c) for i, c in enumerate(chars))\n","# summarize the loaded data\n","n_chars = len(raw_text)\n","n_vocab = len(chars)\n","print (\"Total Characters: \", n_chars)\n","print (\"Total Vocab: \", n_vocab)\n","# prepare the dataset of input to output pairs encoded as integers"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Total Characters:  1364418\n","Total Vocab:  112\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3IkORlGrzpD","executionInfo":{"status":"ok","timestamp":1617989756638,"user_tz":-420,"elapsed":26487,"user":{"displayName":"Tran Trung Kien","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBVSZw8E8uaIf28-VmePyDmzh9gvFKysWrDrAe=s64","userId":"13050720104544302355"}},"outputId":"b0ca5566-aadc-4b4a-a385-0d6bbbd7e124"},"source":["seq_length = 100\n","dataX = []\n","dataY = []\n","print(\"cac\")\n","for i in range(0, n_chars - seq_length, 1):\n","\tseq_in = raw_text[i:i + seq_length]\n","\tseq_out = raw_text[i + seq_length]\n","\tdataX.append([char_to_int[char] for char in seq_in])\n","\tdataY.append(char_to_int[seq_out])\n","n_patterns = len(dataX)\n","print (\"Total Patterns: \", n_patterns)\n","# reshape X to be [samples, time steps, features]\n","X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n","# normalize\n","X = X / float(n_vocab)\n","# one hot encode the output variable\n","y = np_utils.to_categorical(dataY)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["cac\n","Total Patterns:  1364318\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LByV4qm2r2ga","executionInfo":{"status":"ok","timestamp":1617990156925,"user_tz":-420,"elapsed":201699,"user":{"displayName":"Tran Trung Kien","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBVSZw8E8uaIf28-VmePyDmzh9gvFKysWrDrAe=s64","userId":"13050720104544302355"}},"outputId":"520adba7-4e3e-4875-bf5d-4fecc435ae91"},"source":["# define the LSTM model\n","\n","model = Sequential()\n","model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(256))\n","model.add(Dropout(0.2))\n","model.add(Dense(y.shape[1], activation='softmax'))\n","\n","\n","\n","filename = '/content/drive/MyDrive/IT_documents_University/Subjects/F3_semester_ThirdYear/DeepLearning/WordPrediction/weights-improvement-74-1.4272.hdf5'\n","\n","model.load_weights(filename)\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","\n","#predict\n","original_text = ''\n","predicted_text=[]\n","def cls():\n","    print(\"\\n\" * 50)\n","while True:\n","   # print(original_text)\n","    original_text = original_text.replace(\"  \", \" \")  # fix 2 dau cach\n","\n","    text = input(original_text+\" \")\n","    #\n","    #\n","    #\n","    #cls()\n","    print(\"\\n\\n\") \n","    text = text.lower()  ## khoi vang loi uppercase\n","\n","    if text=='`':     #      ~`  > use predict nhu new input\n","        text= predicted_text[len(predicted_text)-1].replace(\" \",\"\")\n","    if text=='//':\n","        break\n","    inp= list(original_text+' '+text)\n","    inp.pop(0)\n","#    print('------------\\n predicted_text: ',predicted_text)\n","#    print('original_text: ', original_text)\n"," #   print('inp: ', inp) ##\n","\n","    last_word = inp[len(original_text):]\n","    inp = inp[:len(original_text)]    \n","    original_text = original_text+' '+text\n","    last_word.append(' ')\n","#    print('inp ',inp)\n","#    print('last_word ',last_word)\n","#    print('original_text: ', original_text)\n","    ########\n","    inp_text = [char_to_int[c] for c in inp]\n","   \n","    last_word = [char_to_int[c] for c in last_word]\n","   \n"," #   print('inp_text ',inp_text)\n","#    print('last_word ', last_word)\n","\n","    if (len(inp_text) > 100):\n","        inp_text = inp_text[len(inp_text)-100: ]\n","    if len(inp_text) < 100:\n","        pad = []\n","        space = char_to_int[' ']\n","        pad = [space for i in range(100-len(inp_text))]\n","        inp_text = pad + inp_text\n","    \n","    while len(last_word)>0:\n","        X = np.reshape(inp_text, (1, seq_length, 1))\n","        next_char = model.predict(X/float(n_vocab))\n","        inp_text.append(last_word[0])\n","        inp_text = inp_text[1:]\n","        last_word.pop(0)\n","       # print(int_to_char[np.argmax(next_char)])\n"," #   print('inp_text ',inp_text)\n","  #  print('last_word ', last_word)\n","    next_word = []\n","    next_char = ':'\n","    while next_char != ' ':\n","        X = np.reshape(inp_text, (1, seq_length, 1))\n","        next_char = model.predict(X/float(n_vocab))\n","        index = np.argmax(next_char)        \n","        next_word.append(int_to_char[index])\n","        inp_text.append(index)\n","        inp_text = inp_text[1:]\n","        next_char = int_to_char[index]\n","    \n","    predicted_text = predicted_text + [''.join(next_word)]\n","    print(\"(Du doan: \" + ''.join(next_word), end=')')\n","    \n","\n","from tabulate import tabulate\n","\n","original_text = original_text.split()\n","predicted_text.insert(0,\"\")\n","predicted_text.pop()\n","\n","table = []\n","dem=0\n","for i in range(len(original_text)):\n","    if (original_text[i].replace(\" \",\"\")==predicted_text[i].replace(\" \",\"\")):\n","        dem=dem+1\n","        table.append([original_text[i], predicted_text[i], str(dem)])\n","    else:\n","        table.append([original_text[i], predicted_text[i], 'flase'])\n","print(tabulate(table, headers = ['Actual Word', 'Predicted Word', 'Resutf']))\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["(Du doan: ngàn ) cuộc  ngàn\n","\n","\n","\n","(Du doan: hoạn ) cuộc ngàn kinh\n","\n","\n","\n","(Du doan: nghiệm ) cuộc ngàn kinh `\n","\n","\n","\n","(Du doan: của ) cuộc ngàn kinh nghiệm `\n","\n","\n","\n","(Du doan: các ) cuộc ngàn kinh nghiệm của `\n","\n","\n","\n","(Du doan: trường ) cuộc ngàn kinh nghiệm của các //\n","\n","\n","\n","Actual Word    Predicted Word    Resutf\n","-------------  ----------------  --------\n","cuộc                             flase\n","ngàn           ngàn              1\n","kinh           hoạn              flase\n","nghiệm         nghiệm            2\n","của            của               3\n","các            các               4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jI_QBQWhtTt9"},"source":["sống \n","quá bế\n"],"execution_count":null,"outputs":[]}]}